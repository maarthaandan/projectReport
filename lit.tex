\chapter{Literature Survey}
ISR is a topic that has captivated scientists for a long time. Earliest methods for Single Image Super Resolution (SISR) were prediction based methods like Bicubic Interpolation and Lanczos filtering. These methods though fast, most often oversimplify the ISR problem and yield solutions with smooth textures. Better approaches establish a complex mapping between low and high resolution information and usually rely on training data. William T Freeman presented a method for ISR that are based on example pairs. Recently convolutional neural network (CNN) based ISR algorithms have shown excellent performance. In “Deep networks
for image super-resolution with sparse prior” by Z.Wang and others, the authors encode a sparse representation prior into their feed-forward network architecture based on the learned iterative shrinkage and thresholding algorithm (LISTA) . In “Image super-resolution
using deep convolutional networks”,C.Dong and C.C Loy used bicubic interpolation to upscale an input image and trained a three layer deep fully convolutional network end-to-end to achieve state- of-the-art SR performance. Subsequently, it was shown that enabling the network to learn the upscaling filters directly can further increase performance both in terms of accuracy and speed. Deeper network architectures have been shown to increase performance for SISR. In “Deeply-recursive convolutional network for image super-resolution” J.Kim, J.K Lee and K.M Lee formulate a recursive CNN and present state-of-the-art results.

The concept of Generative Adversarial Network is explained in detail in the paper by Ian Goodfellow and others titled "Generative Adversarial Nets". The application of Generative Adversarial Nets to the problem of Single Image Super Resolution is discussed in The architecture for this project was derived from a paper, "Photo-Realistic Single Image Super Resolution Using a Generative Adversarial Network" by Christian Ledig, Lucas Theis and others.The architecture for the generator network and the discriminator network was also obtained from this paper. The architecture involves a very important layer called the Batch Normalization layer . In "Accelerating Deep Network Training by Reducing Internal Covariate Shift" by Sergey Ioffe and Christian Szegedy, normalizing layer inputs for solving the problem of internal co-variate shift observed during the training of Deep Neural Networks is discussed in length.
 
In a paper titled "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"  
by Justin Johnson, Alexandre Alahi, and Li Fei-Fei the VGG Loss function is explained in detail with reference to its application for the purpose of style transfer in images. 
In "Deep Face Recognition", authors Omkar Parkhi and others give insight into the process of increasing the size of the dataset efficiently to enable deep neural networks to learn better. The need for batch normalization is debated in "Enhanced Deep Residual Networks for Single Image Super-Resolution" by Bee Lim and others. The paper uses a network architecture without the Batch Normalization layer and achieves comparable results at a much faster rate. 